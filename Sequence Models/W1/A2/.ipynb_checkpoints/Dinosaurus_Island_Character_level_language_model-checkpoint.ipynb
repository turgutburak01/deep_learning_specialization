{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "886671a4-1e4b-49b7-bc30-baf7bad46f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import random\n",
    "import pprint\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b62d69e-4a47-4a3c-9021-0ba0ab77b419",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a10d4d-8962-484a-8f41-800bef7b5ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56a29770-ebca-4ffa-8726-c6aafc77c936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   0: '\\n',\n",
      "    1: 'a',\n",
      "    2: 'b',\n",
      "    3: 'c',\n",
      "    4: 'd',\n",
      "    5: 'e',\n",
      "    6: 'f',\n",
      "    7: 'g',\n",
      "    8: 'h',\n",
      "    9: 'i',\n",
      "    10: 'j',\n",
      "    11: 'k',\n",
      "    12: 'l',\n",
      "    13: 'm',\n",
      "    14: 'n',\n",
      "    15: 'o',\n",
      "    16: 'p',\n",
      "    17: 'q',\n",
      "    18: 'r',\n",
      "    19: 's',\n",
      "    20: 't',\n",
      "    21: 'u',\n",
      "    22: 'v',\n",
      "    23: 'w',\n",
      "    24: 'x',\n",
      "    25: 'y',\n",
      "    26: 'z'}\n",
      "{   '\\n': 0,\n",
      "    'a': 1,\n",
      "    'b': 2,\n",
      "    'c': 3,\n",
      "    'd': 4,\n",
      "    'e': 5,\n",
      "    'f': 6,\n",
      "    'g': 7,\n",
      "    'h': 8,\n",
      "    'i': 9,\n",
      "    'j': 10,\n",
      "    'k': 11,\n",
      "    'l': 12,\n",
      "    'm': 13,\n",
      "    'n': 14,\n",
      "    'o': 15,\n",
      "    'p': 16,\n",
      "    'q': 17,\n",
      "    'r': 18,\n",
      "    's': 19,\n",
      "    't': 20,\n",
      "    'u': 21,\n",
      "    'v': 22,\n",
      "    'w': 23,\n",
      "    'x': 24,\n",
      "    'y': 25,\n",
      "    'z': 26}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i, ch in enumerate(chars)}\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(ix_to_char)\n",
    "pp.pprint(char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c49c44-4075-435a-9d6e-4c4a67fb5342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clipping the gradients in the optimization loop\n",
    "def clip(gradients, maxValue):\n",
    "    gradients = copy.deepcopy(gradients)\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "    \n",
    "    for gradient in [dWax, dWaa, dWya, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out = gradient)\n",
    "        \n",
    "    gradients = {'dWaa': dWaa, 'dWax': dWax, 'dWya': dWya, 'db': db, 'dby': dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c5583f8-17d9-41cd-a1bd-ab45b97d394d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    x = np.zeros([vocab_size,1])\n",
    "    a_prev = np.zeros([n_a,1])\n",
    "    \n",
    "    indices = []\n",
    "    idx = -1\n",
    "    \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        np.random.seed(counter + seed)\n",
    "        \n",
    "        idx = np.random.choice(range(len(y)), p = y.ravel())\n",
    "        \n",
    "        indices.append(idx)\n",
    "        \n",
    "        x = np.zeros([vocab_size,1])\n",
    "        x[idx] = 1\n",
    "        \n",
    "        a_prev = a\n",
    "        \n",
    "        seed += 1\n",
    "        counter += 1\n",
    "        \n",
    "    if counter == 50:\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "        \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b037e9d-ea48-4119-bfcd-9338e444f85f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b36bf2b-556a-42df-be0c-9aaf92f0c1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model(data_x, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27, verbose = False):\n",
    "\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    examples = [x.strip() for x in data_x]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    last_dino_name = \"abc\"\n",
    "    \n",
    "    for j in range(num_iterations):\n",
    "        idx = j % len(examples)\n",
    "        \n",
    "        single_example = examples[idx]\n",
    "        single_example_chars = [char_to_ix[ch] for ch in single_example]\n",
    "        single_example_ix = idx\n",
    "        X = [None] + single_example_chars\n",
    "        \n",
    "        ix_newline = [char_to_ix['\\n']]\n",
    "        Y = X[1:] + ix_newline\n",
    "\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "                \n",
    "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
    "            print(\"j = \" , j, \"idx = \", idx,) \n",
    "        if verbose and j in [0]:\n",
    "            print(\"single_example =\", single_example)\n",
    "            print(\"single_example_chars\", single_example_chars)\n",
    "            print(\"single_example_ix\", single_example_ix)\n",
    "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
    "        \n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                last_dino_name = get_sample(sampled_indices, ix_to_char)\n",
    "                print(last_dino_name.replace('\\n', ''))\n",
    "                \n",
    "                seed += 1 \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters, last_dino_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3ac2237-3a44-4974-a717-3448ca4a37bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j =  0 idx =  0\n",
      "single_example = turiasaurus\n",
      "single_example_chars [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19]\n",
      "single_example_ix 0\n",
      " X =  [None, 20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19] \n",
      " Y =        [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19, 0] \n",
      "\n",
      "Iteration: 0, Loss: 23.087336\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "j =  1535 idx =  1535\n",
      "j =  1536 idx =  0\n",
      "Iteration: 2000, Loss: 27.884160\n",
      "\n",
      "Liusskeomnolxeros\n",
      "Hmdaairus\n",
      "Hytroligoraurus\n",
      "Lecalosapaus\n",
      "Xusicikoraurus\n",
      "Abalpsamantisaurus\n",
      "Tpraneronxeros\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.901815\n",
      "\n",
      "Mivrosaurus\n",
      "Inee\n",
      "Ivtroplisaurus\n",
      "Mbaaisaurus\n",
      "Wusichisaurus\n",
      "Cabaselachus\n",
      "Toraperlethosdarenitochusthiamamumamaon\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.608779\n",
      "\n",
      "Onwusceomosaurus\n",
      "Lieeaerosaurus\n",
      "Lxussaurus\n",
      "Oma\n",
      "Xusteonosaurus\n",
      "Eeahosaurus\n",
      "Toreonosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.070350\n",
      "\n",
      "Onxusichepriuon\n",
      "Kilabersaurus\n",
      "Lutrodon\n",
      "Omaaerosaurus\n",
      "Xutrcheps\n",
      "Edaksoje\n",
      "Trodiktonus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.844446\n",
      "\n",
      "Onyusaurus\n",
      "Klecalosaurus\n",
      "Lustodon\n",
      "Ola\n",
      "Xusodonia\n",
      "Eeaeosaurus\n",
      "Troceosaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.291971\n",
      "\n",
      "Onyxosaurus\n",
      "Kica\n",
      "Lustrepiosaurus\n",
      "Olaagrraiansaurus\n",
      "Yuspangosaurus\n",
      "Eealosaurus\n",
      "Trognesaurus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.382338\n",
      "\n",
      "Meutromodromurus\n",
      "Inda\n",
      "Iutroinatorsaurus\n",
      "Maca\n",
      "Yusteratoptititan\n",
      "Ca\n",
      "Troclosaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.265759\n",
      "\n",
      "Meustoloplohus\n",
      "Imeda\n",
      "Iutosaurus\n",
      "Maca\n",
      "Yuspanenphurus\n",
      "Daaisicachtitan\n",
      "Trodon\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.901372\n",
      "\n",
      "Phytrohaesaurus\n",
      "Melaaisaurus\n",
      "Mystoosaurus\n",
      "Peeamosaurus\n",
      "Ytronosaurus\n",
      "Eiakosaurus\n",
      "Trogonosaurus\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 22.924847\n",
      "\n",
      "Nixusaurus\n",
      "Llecalosaurus\n",
      "Lxusodon\n",
      "Necalosaurus\n",
      "Ystrengosaurus\n",
      "Eg\n",
      "Trochkosaurus\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 22.759659\n",
      "\n",
      "Piustolonosaurus\n",
      "Migbaeron\n",
      "Myrrocepholus\n",
      "Peeadosaurus\n",
      "Yusodomincteros\n",
      "Eiadosaurus\n",
      "Trocephods\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters, last_name = model(data.split(\"\\n\"), ix_to_char, char_to_ix, 22001, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c625c-8479-4c57-8bec-380f9434e1e6",
   "metadata": {},
   "source": [
    "## Writing like Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ed8b176-69fe-4898-86d5-1e3636cc8144",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "Creating training set...\n",
      "number of training examples: 31412\n",
      "Vectorizing training set...\n",
      "Loading model...\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xfc in position 32: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_file\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshakespeare_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Masaüstü\\deep_learning_specialization\\Sequence Models\\W1\\A2\\shakespeare_utils.py:132\u001b[0m\n\u001b[0;32m    130\u001b[0m x, y \u001b[38;5;241m=\u001b[39m vectorization(X, Y, n_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chars), char_indices \u001b[38;5;241m=\u001b[39m char_indices) \n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/model_shakespeare_kiank_350_epoch.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_output\u001b[39m():\n\u001b[0;32m    136\u001b[0m     generated \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:238\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    231\u001b[0m         filepath,\n\u001b[0;32m    232\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    234\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    235\u001b[0m     )\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:703\u001b[0m, in \u001b[0;36mis_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns whether the path is a directory or not.\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;124;03m  True, if the path is a directory; False otherwise\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mIsDirectory(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError:\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xfc in position 32: invalid start byte"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from shakespeare_utils import *\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b98b4-7066-45bb-8ddb-996cb30bb1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
